{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5c427f-e0fe-411e-8866-1596779328f6",
   "metadata": {},
   "source": [
    "# Separador e Extratos de texto de sentenças do Eproc SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6405690-c620-4b7d-adb0-cd5485757eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4e089-6ec8-45f4-a3c2-7dc8d57caed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639db64c-9c5a-4939-9b52-308265c9c240",
   "metadata": {},
   "source": [
    "## Configuração dos Diretórios de Entrada e Saída\n",
    "\n",
    "* O diretório de entrada é o que possui os arquivos PDFs com várias sentenças, e apenas isso\n",
    "* O diretório de saída será onde as sentenças separadas serão armazenadas, um PDF por sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345773a-34d9-44de-b36e-586596a9f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"input\"\n",
    "\n",
    "input_files = os.listdir(input_folder)\n",
    "print(f'Todos arquivos no diretório de entrada: {input_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbf435-1efe-4fb9-87ec-680af933cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove qualquer arquivo que não seja PDF da lista\n",
    "for f in input_files:\n",
    "    if not f.endswith('.pdf'):\n",
    "        input_files.remove(f)\n",
    "\n",
    "print(f'PDFs do diretório de entrada: {input_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ea58d-b4f8-4abb-b233-70eee34d430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório de saída, onde as sentenças separadas serão armazenadas\n",
    "output_folder = \"output\"\n",
    "\n",
    "# Verificar se o diretório não existe\n",
    "if not os.path.exists(output_folder):\n",
    "    # Criar o diretório de saída se não existir\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Diretório de saída: `{output_folder}` criado com sucesso!\")\n",
    "else:\n",
    "    # Se existir, só avisa que já existe\n",
    "    print(f\"O diretório de saída `{output_folder}` já existe!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48f802-48be-46ec-8206-d0c68815358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para imprimir o texto na cor vermelha\n",
    "def red(text):\n",
    "    return f\"\\033[91m{text}\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de44b08-d7d4-427e-9721-c19f95ed4144",
   "metadata": {},
   "source": [
    "## Parte 1 - Divisão das sentenças em arquivos individuais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ac11e-d5c8-4004-9ba0-b13ee6165668",
   "metadata": {},
   "source": [
    "### Processamento de um arquivo de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb19f02-6ce7-470d-8ec9-1e9c0afc17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que lê um arquivo PDF (pdf_reader já aberto) e divide em múltiplos PDFs\n",
    "# de acordo com a lista de páginas (pages) marcadas.\n",
    "# O nome dos arquivos gerados são os números dos processos (sentence_ids)\n",
    "\n",
    "def split_sentence_files(pdf_reader,pages,sentence_ids):\n",
    "    # Marcador da primeira página \n",
    "    start_page = 0\n",
    "    for doc_index, end_page in enumerate(pages):\n",
    "        sentence_id = sentence_ids[doc_index]\n",
    "        output_filename = f\"{output_folder}/{sentence_id}.pdf\"\n",
    "        counter = 1\n",
    "        while os.path.exists(output_filename): # Se o arquivo já existe, ou seja, uma sentença com o mesmo número de processo, gera um nome seguido de um número\n",
    "            print(red(f'O arquivo `{output_filename}` já existe'))\n",
    "            output_filename = f\"{output_folder}/{sentence_id}({counter}).pdf\"\n",
    "            print(f'Criando o arquivo: {output_filename}')\n",
    "            counter += 1\n",
    "        \n",
    "        \n",
    "        print('- Em processamento: [',doc_index+1,\"/\",len(pages),\"] : [\", start_page, \",\", end_page , '] - ', sentence_ids[doc_index], \":\",output_filename)\n",
    "        # Escreve arquivos na saída\n",
    "        pdf_writer = PyPDF2.PdfWriter()\n",
    "        pdf_writer.append(fileobj=pdf_reader, pages=(start_page, end_page))\n",
    "        output_file = open(output_filename, \"wb\")\n",
    "        pdf_writer.write(output_file)\n",
    "        pdf_writer.close()\n",
    "        output_file.close()\n",
    "        \n",
    "        start_page = end_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c8b56-7977-444e-a020-d1530f69c09c",
   "metadata": {},
   "source": [
    "### Processamento em lote dos arquivos de entrada\n",
    "\n",
    "Para cada arquivo de entrada, realiza a operação de divisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b0043-f237-4f98-bf37-ba0b7256b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processamento em lote de todos os arquivos no diretório de entrada!\n",
    "# Para cada arquivo no diretório de entrada, divida-os em múltiplos no diretório de saída!\n",
    "\n",
    "sentence_set = set() # Conjunto para identificar se alguma sentença repete!\n",
    "sentence_ids_files = dict()\n",
    "\n",
    "for file in input_files:\n",
    "    filename = f'{input_folder}/{file}'\n",
    "    print(f'>>>>> Processando arquivo: `{filename}`. AGUARDE...')\n",
    "    # Ler arquivo de entrada\n",
    "    pdf_fileobj = open(filename,'rb')\n",
    "    \n",
    "    # Criar o leitor do arquivo de entrada\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_fileobj)\n",
    "    \n",
    "    # Lista com as marcações das páginas de início e final de sentença\n",
    "    pages_divisors = [] # Vetor com os números das páginas que iniciam cada sentença\n",
    "    sentence_ids = []   # Vetor com o Identificador da sentença que é o número do processo\n",
    "    for i in range(len(pdf_reader.pages)): # Para cada página do arquivo aberto\n",
    "        page_obj=pdf_reader.pages[i]       # Lê a página\n",
    "        page_text=page_obj.extract_text()  # Extrai o texto\n",
    "        page_lines = page_text.split('\\n') # Quebra as linhas\n",
    "        page_first_line = page_lines[0]    # Obtém o texto da primeira linha\n",
    "\n",
    "        # O que identifica a primeira página de cada sentença é o endereço da UFSC (baseado no arquivo de teste)\n",
    "        if page_first_line == 'Av. Des. Vitor Lima, 183, fundos- Campus da UFSC - Bairro: Serrinha - CEP: 88040-400 - Fone: (48)3287-5019 - Email:':\n",
    "            sentence_id = None \n",
    "            for line_index in range(-6,-1): # Busca identificador da sentença nas últimas 6 linhas do texto.\n",
    "                sentence_id = re.match('[0-9]+-[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+',page_lines[line_index])\n",
    "                if sentence_id is not None: # Quando encontrar, para!\n",
    "                    break\n",
    "\n",
    "            if sentence_id.group() in sentence_set:\n",
    "                for sentence_file in sentence_ids_files:\n",
    "                    if sentence_id.group() in sentence_ids_files[sentence_file]:\n",
    "                        print(red(f'A sentença {sentence_id.group()} já está no conjunto, pois apareceu no arquivo: {sentence_file}!'))\n",
    "                \n",
    "            sentence_set.add(sentence_id.group())\n",
    "            pages_divisors.append(i)                  # Marca a página de início da sentença\n",
    "            sentence_ids.append(sentence_id.group())  # O identificador da sentença (número do processo) que está no rodapé da página é registrado\n",
    "\n",
    "    sentence_ids_files[f'{file}'] = sentence_ids\n",
    "    pages_divisors.pop(0) # Remove o marcador de página 0, pois página que inícia a primeira sentença sempre será a primeira\n",
    "    pages_divisors.append(len(pdf_reader.pages)) # Adiciona a última página do arquivo na lista de páginas.\n",
    "\n",
    "    print(f'## Páginas que dividem as sentenças no arquivo `{file}`: {pages_divisors}')\n",
    "    print(f'## Número dos processos do arquivo `{file}`: {sentence_ids}')\n",
    "    split_sentence_files(pdf_reader,pages_divisors,sentence_ids) # Processa o arquivo\n",
    "    pdf_fileobj.close()\n",
    "    print()\n",
    "\n",
    "print(red(\"\\n\\n######## CONCLUÍDO ########\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd515c6-5840-4254-a931-b7506f8f06d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Sentenças encontradas em cada arquivo!')\n",
    "sentence_ids_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74337005-08df-4852-929c-ef10e529cb35",
   "metadata": {},
   "source": [
    "# Parte 2 - Extração de Texto\n",
    "\n",
    "Lê os arquivos gerados no processamento em lote da etapa anterior, extrai o texto dos arquivos e gera um arquvo TXT apenas com o texto útil dos arquivos.\n",
    "\n",
    "O texto útil é resultado da:\n",
    "1. Remoção dos cabeçalhos\n",
    "2. Ignorar rodapé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11320da-fd27-48a6-8327-799acb9076ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o diretório de saída do processamento anterior,\n",
    "# pois é nele que estão todos os PDFs das sentenças individualizadas\n",
    "# para a extração de texto.\n",
    "input_folder = \"output\"  \n",
    "input_files = os.listdir(input_folder)\n",
    "\n",
    "# Remove qualquer arquivo que não seja PDF da lista\n",
    "for f in input_files:\n",
    "    if not f.endswith('.pdf'):\n",
    "        input_files.remove(f)\n",
    "\n",
    "print(f'Há `{len(input_files)}` arquivos para extração de texto no diretório: {input_folder}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce89b46-7399-4862-ab5e-ebd07a471bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A spaCy não foi adotada na estratégia de geração manual, pode ser ignorada\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c844c-736e-458f-adcb-e55a5331fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "nlp = Portuguese()\n",
    "# Criando o objeto spacy\n",
    "#nlp = spacy.load(\"pt_core_news_sm\")\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.add_pipe('sentencizer')\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # nlp object is used to create documents with linguistic annotations\n",
    "    doc = nlp(text)\n",
    "    sentence_list =[]\n",
    "    for sentence in doc.sents:\n",
    "        sentence_list.append(sentence.text)\n",
    "    \n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb02e8-1e38-4d53-880f-d890ba3ec098",
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_numbers = ('I.','II.','III.','IV.','V.','VI.','VII.','VIII.','IX.','X.', \n",
    "                 'XI.','XII.','XIII.','XIV.','XV.','XVI.','XVII.','XVIII.','XIX.','XX.',\n",
    "                 'XXI.','XXII.','XXIII.','XXIV.','XXV.','XXVI.','XXVII.','XXVIII.','XXIX.','XXX.',\n",
    "\n",
    "                 'I -','II -','III -','IV -','V -','VI -','VII -','VIII -','IX -','X -', \n",
    "                 'XI -','XII -','XIII -','XIV -','XV -','XVI -','XVII -','XVIII -','XIX -','XX -',\n",
    "                 'XXI -','XXII -','XXIII -','XXIV -','XXV -','XXVI -','XXVII -','XXVIII -','XXIX -','XXX -'\n",
    "                )\n",
    "\n",
    "for file_index, file in enumerate(input_files):\n",
    "    filename = f'{input_folder}/{file}'\n",
    "    print(f'>>>>> Processando arquivo [{file_index+1}/{len(input_files)}]: `{filename}`. AGUARDE...')\n",
    "    # Ler arquivo de entrada\n",
    "    pdf_fileobj = open(filename,'rb')\n",
    "    \n",
    "    # Criar o leitor do arquivo de entrada\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_fileobj)\n",
    "\n",
    "    txt_content = ''\n",
    "    txt_content2 = ''\n",
    "    txt_content3 = ''\n",
    "    \n",
    "    piece_text = ''\n",
    "    for p in range(len(pdf_reader.pages)): # Para cada página do arquivo aberto\n",
    "        page_obj = pdf_reader.pages[p]      # Lê a página\n",
    "        page_text = page_obj.extract_text() # Extrai o texto\n",
    "        page_lines = page_text.split('\\n')  # Quebra as linhas\n",
    "\n",
    "        \n",
    "        for line,text in enumerate(page_lines):\n",
    "            if p == 0: # Primeira página do documento\n",
    "                if line in [0,1]: # Primeiras duas linhas da primeira página: Cabeçalho com informações da UFSC\n",
    "                    continue\n",
    "                if line == 2:      # Terceira linha da primeira página: Informação do Tribunal e Número do Processo\n",
    "                    txt_content += text.replace(\"PROCEDIMENT O\", \"PROCEDIMENTO\") + '\\n\\n'\n",
    "                    txt_content2 += text.replace(\"PROCEDIMENT O\", \"PROCEDIMENTO\") + '\\n\\n'\n",
    "                    txt_content3 += text.replace(\"PROCEDIMENT O\", \"PROCEDIMENTO\") + '\\n\\n'\n",
    "                    continue\n",
    "                if text.strip().lower().startswith(('réu','autor','sentença')):\n",
    "                    txt_content += text + '\\n\\n'\n",
    "                    txt_content2 += text + '\\n\\n'\n",
    "                    txt_content3 += text + '\\n\\n'\n",
    "                    continue\n",
    "\n",
    "            if re.match('^([0-9]+-[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)',text): # Encontrou o ID do processo no início da linha\n",
    "                break          # Ao encontrar o ID de sentença (número do processo que fica no final de cada da página) - para de processar a página\n",
    "\n",
    "            if text.startswith(roman_numbers) or re.match('^([0-9]+\\.){1,3} ',text):\n",
    "                if piece_text:\n",
    "                    #print(\"\\nImprimindo pedaço\")\n",
    "                    #print(piece_text)\n",
    "                    #print(red('Sentenças'))\n",
    "                    sentences = tokenize_sentences(piece_text)\n",
    "                    txt_content2 += '\\n'.join(sentences)\n",
    "                    #print(sentences)\n",
    "\n",
    "                txt_content += f'\\n\\n{text}\\n\\n'\n",
    "                txt_content2 += f'\\n\\n{text}\\n\\n'\n",
    "                txt_content3 += f'\\n\\n{text}\\n\\n'\n",
    "                \n",
    "                piece_text = ''\n",
    "                continue\n",
    "\n",
    "            if p == len(pdf_reader.pages)-1: # Se for a última página\n",
    "                if text.startswith('Signatário'):\n",
    "                    txt_content += f'\\n{text}\\n'  # Para pegar apenas o nome do Juiz: text.split(\":\")[1].strip()\n",
    "                    sentences = tokenize_sentences(piece_text)\n",
    "                    txt_content2 += '\\n'.join(sentences)\n",
    "                    txt_content2 += f'\\n{text}\\n'\n",
    "\n",
    "                    txt_content3 += f'\\n{text}\\n'\n",
    "                    continue\n",
    "            piece_text += text + ' '\n",
    "            txt_content += text + '\\n'\n",
    "            if re.match('^[A-Z]',text):\n",
    "                if txt_content3.rstrip().endswith(('.', ':')):\n",
    "                    txt_content3 += '\\n' + text\n",
    "                else:\n",
    "                    txt_content3 += ' ' + text\n",
    "            else:\n",
    "                txt_content3 += ' ' + text\n",
    "                    \n",
    "            #print(f'[{line}] : {text}')\n",
    "        # Processar linha-a-linha\n",
    "        #print()\n",
    "\n",
    "    pdf_fileobj.close()\n",
    "\n",
    "#    f = open(f'{input_folder}/{file[:-4]}-all.txt', \"w\") # Gerar versão linha-a-linha separada, sem nenhum tratamento.\n",
    "#    f.write(txt_content)\n",
    "#    f.close()\n",
    "#    f = open(f'{input_folder}/{file[:-4]}-v2.txt', \"w\")  # Gerar separação com tokenizador PT da spaCy\n",
    "#    f.write(txt_content2)\n",
    "#    f.close()\n",
    "    f = open(f'{input_folder}/{file[:-4]}.txt', \"w\")     # Gerar tentativa manual de formatação.\n",
    "    f.write(txt_content3)\n",
    "    f.close()\n",
    "#    print(f'TXT arquivo [{file}]: \\n{txt_content}')\n",
    "\n",
    "print(red('##### CONCLUÍDO #####'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465671a-ffab-4cc6-bff5-6bf2096cecb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
